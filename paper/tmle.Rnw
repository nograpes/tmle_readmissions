\documentclass[]{article}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref} % For linking Github

\usepackage[left=1.27cm, top=1.27cm, right=1.27cm, bottom=2.54cm]{geometry}

\usepackage{pbox}
\setmainfont[Scale=1]{Cambria}
\linespread{1.25}
\usepackage{pdflscape}

\usepackage{setspace}
\usepackage[document]{ragged2e} % For left-alignment.
\usepackage{parskip} % For space between paragraphs

% If you use Biber, then you will have to compile and recompile.
% But Biber seems to be the preferred choice.
\usepackage[backend=biber,sorting=none,style=numeric-comp]{biblatex}

\addbibresource{tmle.bib} % Critical that you put .bib in here!

\begin{document}
\title{Hospital readmissions and targeted maximum likelihood estimation}
\author{Aman Verma}
\date{\today}

\begin{abstract}

<<formatterFunctions, include=FALSE>>=
  num <- function(x) format(x, big.mark=',')
@


<<preChunk, include=FALSE>>=
  load(file='~/repo/thesis/code/tmle/tables/disease.results.table.object') # disease.results.table

  readmitted <- sapply(disease.results.table, function(x) sum(x$readmitted))
  discharged <- sapply(disease.results.table, function(x) sum(x$live.discharge))

  readmission.range <- function(disease)
    paste0('(',paste(round(range(disease.results.table[['ami']]$prop)*100),collapse='-'),'%)')

  readmission.props <- function(disease)
                paste0(paste0(num(readmitted[disease]),' / ',num(discharged[disease])),
                      ' (',round(readmitted[disease]*100 / discharged[disease],0),'\\%)')

@


Background: Hoping to improving quality of hospital care, the US and other jurisdictions financially penalize hospitals with poor (confounder-adjusted) 30-day readmission rates. Although hospital administrative data is information-rich, confounder adjustment tends to be crude. Non-parametric machine learning techniques can take advantage of these rich data to predict readmission, but cannot isolate the independent effect of hospitals on readmission risk.

Research Design: To estimate the effect of care at different hospitals on 30-day readmission risk, we used targeted maximum likelihood estimation (TMLE), which allowed us to use a non-parametric machine learning technique (random forest) to take advantage of the rich confounder data. We used an 11-year cohort of 65-year-old patients from 20 hospitals in Montreal, Canada, and developed three models to estimate the marginal readmission risk at each of the hospitals after hospitalization for heart failure, acute myocardial infarction (AMI), and pneumonia. We controlled for hundreds of confounders including outpatient drug prescriptions, medical procedures, and diagnoses.

Results: Within 30 days of discharge, there were \Sexpr{readmission.props('heart_failure')} heart failure readmissions, \Sexpr{readmission.props('pneumonia')} pneumonia readmissions, and \Sexpr{readmission.props('ami')} AMI readmissions. Within each hospital, there was a wide variation in crude readmission risk across the twenty hospitals for pneumonia \Sexpr{readmission.props('pneumonia')}, heart failure \Sexpr{readmission.props('heart_failure')}, and AMI \Sexpr{readmission.props('ami')}. When crudely controlling for confounding, the marginal risk for readmission within all hospitals was nearly the same, but when we applied TMLE, we found significant differences in the effect of different hospitals on readmission risk.

Conclusion: Our results suggest that current estimates of the effect of hospitals on 30-day readmission risk may be not be sufficient to identify low quality of care, and that TMLE with machine learning methods may reveal these differences.

\end{abstract}

\section{Introduction}
% Why is this question important to answer?
In the early eighties, hospital administrators in the US sought to reduce hospitalization costs by changing the reimbursement system. Instead of paying hospitals per day of hospitalization, hospitals were paid a fixed rate for the type of hospitalization and the procedures performed.\supercite{iglehart_medicare_1983} Following implementation of this law, the length of stay at hospitals dropped dramatically, although evidence exists that is was already in decline.\supercite{gornick_medicare_1977}

% Pay-per-service creates a different perverse incentive, leading to calls for quality of care metrics.
% I need to add the penalty.
% The three initial conditions selected by the Centers for Medicare and Medicaid Services (CMS) to implement the Hospital Readmissions Reduction Program mandated by the Affordable Care Act
Some worried that the new system created a perverse incentive to discharge patients early, and admit them again at a later date.\supercite{anderson_hospital_1984} To ensure proper quality of care in the hospitals while keeping costs controlled, administrators have sought to establish useful quality of care metrics.\supercite{institute_of_medicine_u.s.._division_of_health_care_services_medicare_1990} Hospital readmissions have been identified as a simple metric that can establish a baseline of care; if an abnormally high number of patients from a certain hospital are quickly readmitted, it could indicate poor quality of care. To measure quality of care, administrators have selected three common admission reasons with high readmission risk: acute myocardial infarction (AMI), pneumonia, and heart failure.\supercite{us_government_public_2012}

% To use hospital readmissions as a QoC metric, we must control for confounding.
Since hospitals admit patients with varying risk of readmission, it is important to accurately estimate the effect of hospital treatment on readmission, independent of patient-level confounders. Without effectively controlling for confounding, we risk unfairly penalizing hospitals that treat sicker (more likely to be readmitted) patients. Fortunately, hospital and outpatient administrative data is information-rich. Drug prescriptions, diagnoses, and medical procedures can provide important information on how the effect of hospital care on readmission risk is confounded by patient health.

% Studies typically control for confounding by using the Charlson comorbidites.
However, in most statistical models of readmission risk, the hospital administrative data is simplified to a few well-known confounders (age, sex, previous readmissions), and sometimes a summary "comorbidity score".\supercite{kansagara_risk_2011} If each drug, diagnosis and procedure was modeled with a separate covariate, the model would be very computationally expensive to fit. Such a model would also be very unwieldy to develop; manually analyzing how inclusion or exclusion of variables affects the model would be impossible to do effectively with hundreds of covariates.

% But Charlson is based on expert knowledge, we have better data.
By summarizing confounders into crude risk scores, we risk "residual confounding" leading to biased effect estimates. Furthermore, to compare hospitals, we are only interested in estimating one parameter, meaning that the independent effect of hospitals on readmissions. We are only interested in the other variables insofar as they confound the effect of hospitals on readmissions, estimating the individual effect of each of these variables is not strictly necessary.

% We can use machine learning techniques to develop a model with thousands of covariates.
Non-parametric machine learning techniques can accurately discriminate patient readmission risk using hundreds of variables in a computationally efficient way.\supercite{friedman_regularization_2010} Because non-parametric are flexible, we avoid having to specify a functional form, and are more likely to detect complex relationships like multi-way interactions. On the other hand, non-parametric techniques don't allow us to isolate (target) the effect of specific variables (such as care at a particular hospital) on readmission risk.

% Instead of summarizing comorbidity scores using expert knowledge, we use the data to develop a comorbidity score specific to the problem at hand.
The targeted maximum likelihood estimator (TMLE), is a doubly-robust technique that uses propensity scores to estimate target parameters of interest, and allows the incorporation of machine learning techniques.\supercite{van_der_laan_targeted_2011} To use TMLE, a model is developed to estimate the probability of exposure (the propensity score), and also fit another model to estimate the probability of the outcome. These two probabilities are combined in a parametric model with only the parameter of interest, inversely weighted by the probability of exposure, and offset by the probability of the outcome. In this way, the discriminative power of non-parametric models can be used to extract estimates of parameters of interest.

% What is the study question?
Although some studies have used the rich confounder data in combination with machine learning techniques to predict hospital readmissions, no study to our knowledge has used these data to draw causal inference on the effect of quality of care on readmissions. In this study, we sought to estimate the independent effect of hospital care on the 30-day readmission for twenty Montreal hospitals, within three different admission diagnoses (pneumonia, heart failure, and acute myocardial infarction). We used a non-parametric machine learning technique, (random forest\supercite{breiman_random_2001}), with TMLE to take advantage of the rich confounder data and minimize bias in our estimate of readmission risk.

\section{Methods}

\subsection{Study Design}
% Cohort selection
We used a cohort extracted from a Canadian provincial (Quebec) administrative database of hospitalizations, obtained from the \emph{Régie de l'assurance maladie du Québec} (RAMQ). We enrolled patients into this cohort on the month that two conditions were satisfied: 1) they had at least one diagnosis of a respiratory illness (the exact list of respiratory International Classification of Diseases, 9th Revision [ICD-9] codes is given in the Appendix) between January 1st, 1996 and March 31, 2006 (the study period), while living in the 2006 census metropolitan area of Montreal, and 2) were at least 65 years of age. We used this cohort because it represents the majority of 65-year-old patients who were hospitalized in the region during the study period.

From among this cohort, we selected hospital discharges for those who had accrued at least one continuous year in the cohort preceding the time of admission. We restricted our data to only the discharges from the twenty hospitals with the most discharges of patients 65 years of age or older within the study period; the twenty hospitals accounted for 75\% of all such discharges.  We only selected hospital discharges which resulted from hospital stays of at least one day. Therefore, the earliest possible hospital discharge was January 2, 1997.

% Disease types
From among the identified hospital discharges, we selected only those with one of three high-volume admission diagnoses with high rates of hospital readmissions: pneumonia, acute myocardial infarction (AMI), and heart failure. We identified each of the admission diagnoses using ICD-9 codes; for pneumonia we used codes ranging from 480-487, for heart failure we used all 428 codes, and for AMI we used all 410 codes. The following methods were applied individually to all three disease subsets.

\subsection{Hospital readmissions}
The unit of analysis was the hospital discharge; a person could be discharged multiple times. A hospital readmission was defined as an emergency hospital admission to any Quebec hospital in the 30 days following a discharge.  A person who died or had a non-emergency readmission in the 30 days following discharge was considered not readmitted.

We defined a preventable readmission in terms of the unobservable counterfactual, a readmission that would not have occurred if the patient was treated at a different hospital. In our statistical analysis, for each patient and hospital combination, we estimated the probability of readmission. By comparing each hospital's effect on the risk of readmission of each patient, we estimated the proportion of preventable readmissions.

\subsection{Confounders and Risk Factors}
% The basic demographics and time-related confounders.
For each hospital discharge, we collected plausible confounders that measured states at the time of, or prior to, admission. We used the demographic characteristics (age at time of admission (years), sex, birth year-month), the number of previous readmissions (within the preceding year), the admission diagnosis (as measured by the specific ICD-9 code). We also included the day of week of discharge, which has been previously shown to have an association with readmissions\supercite{van_walraven_risk_2002}, and the month of discharge, because we hypothesized that readmission risk would vary by season in Montreal.

% Procedures, drugs, and diagnoses.
Additionally, for each discharge, we collected the Quebec hospital diagnoses, Quebec hospital procedures, and drugs dispensed outside of the hospital but inside Quebec, in the year preceding the admission. The hospital procedures were recorded in the Canadian Classification of Diagnostic, Therapeutic, and Surgical Procedures (CCP) system. Hospital diagnostic codes were coded using the ICD-9 system. Finally, drugs which were prescribed and dispensed outside the hospital, and were being taken on the day of admission were also recorded for each patient in the \emph{code commune} system, which categorizes drugs based on the chemical compound. To ease computation, before fitting any model, we removed any diagnosis, procedure or drug that occurred less than 30 times among all discharges. We chose 30 because it appeared to be a natural breakpoint; if the number of variables included is a function $f$ of the threshold, then the first derivative of $f$ dropped at 30 for all three disease categories.

% Area of residence.
We believed that residential location would strongly affect the probability of admission to the hospital nearest that census tract. We included it in our models because we also expected it to crudely approximate a (expected) confounder: socio-economic status.  We used the residential postal code at the time of admission to assign each patient in the cohort to a census tract, as defined by the 2006 Canadian census. (Census tracts contain between 2,500 and 8,000 people, and, at the time of their creation, are demarcated so as to maximize homogeneity of socioeconomic characteristics.)\supercite{statistics_canada_2006_2007}

\subsection{Statistical Analyses}
For each discharge $i$, we sought to estimate the effect of each of the twenty hospitals $A \in \left\{ {hosp_1, \dots ,hosp_{20}}\right\}$ on 30-day readmission ($Y$), accounting for the vector of confounders ($W$). To estimate this risk, we used targeted maximum likelihood estimation, which consisted of several steps. We first estimated of a model of the propensity score $g=Pr(A|W)$ (using random forest described below). Next, we estimated of a model of readmission risk based on the confounders $W$ and the variables for each of the hospitals $Q=Pr(Y=1|A,W)$. We then calculated $h_a(A,W)$ (sometimes referred to as the clever covariate) described in equation \ref{logistic_clever_covariate}

\begin{equation}
\label{logistic_clever_covariate}
h_a(A,W)=\frac{I(A=a)}{g(a|W)}
\end{equation}

(where $I$ is the indicator function which evaluates to 1 when its argument is true, and 0 otherwise), and solved for all ${\epsilon}_a$ in fluctuation function described in equation \ref{logistic_fluctuation_function}.

\begin{equation}
\label{logistic_fluctuation_function}
Y_i=expit(logit(Q(Y_i|A_i,W_i)) + \sum_{a=hosp_1}^{hosp_{20}} {\epsilon}_a \times h_a(A_i,W_i))
\end{equation}

We solved for all twenty ${\epsilon}_a$ by regressing the 30-day readmission outcome $Y$ (with a logit link function) onto $h_a(A,W_i)$ (with no intercept) offset by the inverse logit of the initial estimate of readmission risk $Q=(Y|A,W)$. Finally, for each discharge, we computed the estimated risk of 30-day readmission for all twenty counterfactual conditions (the risk of readmission for every discharge as if they had attended different hospital) using Equation \ref{Q_star_logistic}.

\begin{equation}
\label{Q_star_logistic}
Q^*_{ai}= expit(logit(Q(Y|a,W_i)) + \frac{\epsilon}{g(a|W)})
\end{equation}

For each hospital, we then calculated the mean readmission risk ($Q^*_a$) and associated odds ratio.

% Random forest
To estimate both models $g(A_i|W_i)$ and $Q(Y_i|A,W_i)$, we used a random forest, a non-parametric model based on decision trees.\supercite{breiman_random_2001} Decision trees use the independent variables ($W_i$) to repeatedly split data into partitions that are as homogeneous as possible with respect to the outcome of interest (specifically measured with the Gini coefficient\supercite{gini_variabilita_1912}). Random forest improves decision trees by using bootstrap aggregation (bagging); multiple decision trees are grown on bootstrap replicates (sampled with replacement) to avoid overfitting. Additionally, within each tree, only a sample of the covariates is used (in our case we used a square root of the number of variables included in the mode, rounded down).

For both models $g(A_i|W_i)$ and $Q(Y_i|A,W_i)$, we arbitrarily chose to grow 1200 trees, and then measured the accuracy as a function of the number of trees to ensure that growing further trees would be unlikely to improve accuracy. Because the model was used solely to estimate the \emph{probability} of admission to to specific hospitals (and not to predict exactly which hospital was attended), when calculating the Gini coefficient to build the trees we configured the model to favor calibration over discrimination: we weighted each of the twenty predicted hospitals by the inverse of the proportion of discharges at that hospital. When measuring the accuracy for each discharge, we only used trees for which the discharge was "out-of-bag", that is, we only used trees for which the bootstrap sample did not include the discharge.

% Variable importance
To describe importance of the covariates in both models $g(A_i|W_i)$ and $Q(Y_i|A,W_i)$, for each variable, we measured the decrease in the Gini coefficient for each partition in which the variable was used, in every tree. A low Gini (i.e. higher decrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes. We plotted the densities of variables with four different classes (census tract, procedure, diagnosis and drug) at different levels of Gini decreases.

% Calibration
Random forest classifies each item by majority vote. Although the vote proportion is in the scale of zero to one, it is not calibrated well as a probability; to calibrate the vote proportion, we used Platt scaling\supercite{platt_probabilistic_1999} (logistic regression of the outcome ($Y_i$) on to the vote proportion).

% gbound
When the probability of exposure $g(A|W)$ is very low, that discharge would receive a large weight in estimating $Q^*$. For any $g(A|W)$ below some fixed value $\delta$, we set $g(A|W)$ to $\delta$. We recomputed our analyses at 31 different values of $\delta$, ranging from $10^{-2}$ to $10^{-5}$, decreasing the exponent at intervals of 0.1.

% Comparison to assessment with comorbidity scores
Finally, we compared our results of our analysis with a logistic regression for 30-day readmission. In this model, we included only the age, sex, number of previous admissions, and the Charlson comorbidity score (Elixhauser version)\supercite{elixhauser_comorbidity_1998}, along with indicator variables representing the hospitals themselves.

\subsection{Software}
The data were cleaned and prepared for statistical analysis using the Postgres relational database (version \Sexpr{system("psql --version | rev | cut -d' ' -f 1 | rev", intern=TRUE)}). We implemented our models using the R statistical package (version \Sexpr{getRversion()}).\supercite{team_r:_2014} We implemented the random forest using the "bigrf" package (version \Sexpr{packageVersion('bigrf')}).\supercite{lim_bigrf:_2014} We plotted our figures using the "ggplot2" package (version \Sexpr{packageVersion('ggplot2')}) .\supercite{wickham_ggplot2:_2009} All the code to develop used to process our data, fit our models, and typset this article is \href{https://github.com/nograpes/tmle_readmissions}{available for download at Github}.

\section{Results}
<<simple_stats, cache=TRUE, echo=FALSE>>=
  prefixes<-c('ami','heart_failure','pneumonia')
  pretty.names<-c('Acute myocardial infarction','Heart failure','Pneumonia')
  names(pretty.names)<-prefixes

  get.data<-function(prefix) {
    dump.dir<-'../data_dump/'
    data.files<-'disease_'
    files<-paste0(dump.dir, data.files, prefix, '.object')
    e<-new.env()
    for(file in files) load(file,envir=e)
    mget(ls(e),envir=e)
  }

  models <- sapply(prefixes, get.data, simplify=FALSE)
  readmitted.to.same.hosp <- prop.table(rowSums(sapply(models,function(x) {
    y <- x$disease.df[x$disease.df$day_30_readmit,]
    table(as.character(y$hosp)==as.character(y$readmission_hosp))
  })))['TRUE']

  ever.admitted <- lapply(models ,function(x) length(unique(x$disease.df$id)))
  n<-lapply(models ,function(x) nrow(x$disease.df))

  means <- lapply(models,function(x) round(mean(table(x$disease.df$id)),1))
  medians <- lapply(models,function(x) median(table(x$disease.df$id)))

  rm(models) # So it doesn't get cached.
 @

Over the course of January 2, 1996 to March 31, 2006, \Sexpr{num(482064)} people were entered into our cohort. Among these, \Sexpr{num(ever.admitted$pneumonia)} were ever admitted for pneumonia, \Sexpr{num(ever.admitted$ami)} were ever admitted for AMI, and \Sexpr{num(ever.admitted$heart_failure)} were ever admitted for heart failure. People ever admitted for pneumonia had a mean (median) \Sexpr{num(means$pneumonia)} (\Sexpr{num(medians$pneumonia)}) pneumonia admissions, heart failure patients had a mean (median) \Sexpr{num(means$heart_failure)} (\Sexpr{num(medians$heart_failure)}) heart failure admissions, and AMI patients had a mean (median) \Sexpr{num(means$ami)} (\Sexpr{num(medians$ami)}) AMI admissions. In total, we analyzed \Sexpr{num(n$pneumonia)} pneumonia discharges, \Sexpr{num(n$ami)} AMI discharges, and \Sexpr{num(n$heart_failure)} heart failure discharges.

% The accuracy of random forest.
% The types of variables that were important for both g and Q.
The accuracy of the random forest (for both models $g$ and $Q$) did not appear to improve significantly beyond 125 trees (see figure \ref{fig:error_rate_for_hospital_choice} in the appendix). In figure \ref{fig:variable_importance_by_model_and_class} we plot the importance of variables (as measured by the Gini coefficient) in the random forest models for four variable classes, for all disease subsets for both the $g$ and $Q$ model. Although census tracts were found to be important in prediction of hospital choice,  the other three variable classes were had a high density of important variables as well. The prescription drugs in particular had a high proportion of important variables, and generally the lowest proportion of unimportant variables. For the $Q$ model, the variable density appeared bimodal within variable importance for all four variable classes. Additionally, the pre-admission drug prescriptions appeared to be strongly important in predicting readmission for all pneumonia, heart failure, and AMI admissions.

% The distribution of g
The predicted probability of admission to any particular hospital ($g=Pr(A=a|W)$) was less than 5\% in 88\% of cases (across all disease subsets and hospitals).  We set $\delta$ (the lower bound of $g$ when used to fit the $\epsilon$ values for $Q^*$), to two different values, $10^{-2}$ and $10^{-2.5}$. Across all disease subsets and hospitals, 39\% of discharge/hospital combinations and a $g$ less than $10^{-2}$, and 4\% had a $g$ less than $10^{-2.5}$. Figure \ref{fig:dist_g_zoomed} (in the Appendix) describes the histogram of $g$ when it is below 0.05 for each disease/hospital combination separately.

% The tables
% The in hospital death rates
% The crude model.
% The TMLE models.
The unadjusted proportion of patients readmitted in 30 days varied across hospitals for each disease subset (Tables 1-3). The linear correlation between the proportion of deaths during hospital stay and the proportion readmitted was (\Sexpr{round(death.and.readmissions.cor[c('ami','heart_failure','pneumonia')],2)}) among AMI, heart failure, and pneumonia admissions respectively. Using a model that adjusts for a few well-known confounders, for AMI, heart failure, and pneumonia respectively, one, three, and five hospitals had significantly different odds than the reference hospital. Notably, the significant odds ratios are all relatively small, with point estimates ranging from 0.92 - 1.04. In contrast, in the TMLE models, at both values of $\delta$, for all admission diagnoses, nearly all of the hospitals had significantly different odds than the reference hospital.

% Effect of delta
In some hospitals and disease subsets, the parameter $\delta$, (the lower bound on the probability of exposure $g(A|W)$) had a considerable effect on the marginal risk and the associated odds ratios. For example, for AMI (shown in Table 1), the marginal risk for hospital 17 increases by six percent when $\delta$ decreases from $10^{-2}$ to $10^{-2.5}$. In figure \ref{fig:effect_of_gbound}, we display the marginal risk for each of the twenty hospitals and disease subsets as a function of the parameter $\delta$. For many hospitals, the effect was quite strong; for pneumonia admissions, hospital 16 went from having the second-lowest marginal risk when $\delta=0.1$ to having the highest marginal risk when $\delta=0.025$.


% Variable importance.
\begin{figure}[H]
    \includegraphics{../figures/variable_importance_by_model_and_class.png}
    \caption[Error rate for random forest model of hospital choice.]
{Variable importance by model and variable class. For each random forest classifier, the variable importance was measured by the decrease in the Gini coefficient when that variable splits a node. The horizontal axis within each panel is displayed on a $log_e$ scale. Some variables had exactly zero importance; to avoid evaluating the logarithm of zero, we added a small constant ($e^{-12}$) to the measure of variable importance. The vertical axis in each panel represents the variable density at the corresponding level of variable importance. To transform the individual variable importances into a continuous density, we smoothed using a Gaussian kernel density estimator, using Silverman's 'rule-of-thumb' \supercite{silverman_density_1986} to select the bandwidth. The density is measured separately for each class; the area under each variable class curve is exactly one.
}
   \label{fig:variable_importance_by_model_and_class}
\end{figure}

% Table code.
\begin{landscape}
\setmainfont[Scale=1]{Cambria}
\linespread{1}
<<disease_tables, cache=FALSE, echo=FALSE, results='asis'>>=
suppressPackageStartupMessages(library(Hmisc))
load(file='~/repo/thesis/code/tmle/tables/disease.results.table.object') # disease.results.table

diseases <- c('ami','heart_failure','pneumonia')
pretty_names <- c(ami='Acute myocardial infarction (AMI)', heart_failure='Heart failure', pneumonia='Pneumonia')
pretty_names_lower <- c(ami='acute myocardial infarction (AMI)', heart_failure='heart failure', pneumonia='pneumonia')

ci.format <- function(x){
  baseline <- which(apply(x,1,function(x) all(is.na(x))))
  x=lapply(x,function(y) sprintf('%.2f',round(y,2)))
  l <- do.call(paste0,list(x[[1]],' (',x[[2]],'-',x[[3]],')'))
  l[[baseline]]<-'(Reference)'
  l
}

for (disease in diseases){
  table<-disease.results.table[[disease]]
  odds <- ci.format(table[c('odds.ratio','odds.ratio.ci.low','odds.ratio.ci.high')])

  tmle.odds <- do.call(cbind,lapply(which(colnames(table)=='or',), function(x)
    data.frame(tmle.odds=ci.format(table[x:(x+2)]),Q.star=table[,x+3])))

  pre.format <- data.frame(table[c('admitted',
                              'died',
                              'died.prop',
                              'live.discharge',
                              'readmitted',
                              'prop')],
                      odds=odds,
                      crude.marginal=table$marginal.risk,
                      tmle.odds,
                      check.names=FALSE
                      )

  num.cols <- sapply(pre.format,is.numeric)
  pre.format[num.cols] <- round(pre.format[num.cols],2)

  pre.format$died <- paste0(pre.format$died," (",pre.format$died.prop*100,")")
  pre.format$died.prop <- NULL
  pre.format$readmitted <- paste0(pre.format$readmitted," (",pre.format$prop*100,")")
  pre.format$prop <- NULL

  colnames(pre.format)<-c(admitted='Admitted',
                          died='Died\n(\\%)',
                          live.discharge='Discharged',
                          readmitted='Readmitted\n(\\%)',
                          prop='',
                          odds='Odds ratio\n(95\\% CI)',
                          crude.marginal='Marginal\nrisk',
                          tmle.odds='Odds ratio\n(95\\% CI)',
                          Q.star='Marginal\nrisk'
                        ) [colnames(pre.format)]
  rownames(pre.format) <- NULL

  tmle.colnames <- paste0("TMLE ($\\delta=10^{", c(-2,-2.5),"}$)")

  caption.text = "The proportion of those who were readmitted within 30 days is caluculated using the number discharged alive as the denominator. The confidence intervals for the odds ratios for the parameters in the logistic regression model were calculated using the profile likelihood method.\\supercite{cox_analysis_1970} The marginal risk for the odds ratios was calculated by using the regression model to calculate the mean predicted probability of readmission for every admission, except individually fixing the hospital attended to one hospital. The parameter $\\delta$ represents the lower bound on the probability of exposure to that hospital ($g$); we display odds ratios and marginal risks for two versions of the TMLE model with varying levels of $\\delta$ ."

  caption.text.see.other.table = paste0("The columns in this table are described in Table \\ref{",paste(diseases[1], 'table', sep='_'),"}.")


  latex(pre.format, file='',
        col.just = rep('r',10),
        cgroup = c("", "Logistic regression", tmle.colnames),
        n.cgroup = c(4,2,rep(2,length(tmle.colnames))),
        rowlabel = "Hsp.",
        caption.loc = "bottom",
        caption = paste('Risk of 30-day readmission after admission for ',pretty_names_lower[disease],' in twenty Montreal hospitals. ',ifelse(disease==diseases[1], caption.text, caption.text.see.other.table)),
        label = paste(disease, 'table', sep='_')
        )
}
@
\end{landscape}
\begin{figure}[]
    \centerline{
      \includegraphics{../figures/effect_of_gbound.png}
    }
    \caption[Effect of $\delta$ (the bound on $g(A|W)$) on the marginal risk.]
      {Effect of $\delta$ (the bound on $g(A|W)$) on the marginal risk ($Q^*$). The vertical axis represents the marginal risk as calculated by the TMLE model. The marginal risk ($Q^*$) was evaluated at 31 levels of $\delta$, from $10^{-1}$ to $10^{-5}$, (the exponent decreasing by 0.1). Note that the scale of the horizontal axis decreases from left-to-right. The hatched vertical lines mark the two levels of delta displayed in tables \ref{ami_table},\ref{heart_failure_table} and \ref{pneumonia_table}.}
    \label{fig:effect_of_gbound}
\end{figure}

\section{Discussion}
Using targeted maximum likelihood estimation (TMLE) to adjust precisely for measured confounders, we found that the differences in marginal risk of 30-day hospital readmission in twenty Montreal hospitals were much stronger than a model that only crudely adjusted for confounding readmission risk suggested. Additionally, our study revealed some practical positivity violations for some hospitals, suggesting that the relative readmission risk may not always be estimable from observed data.

% How does this compare to previous research?
% Our study had several strengths: doubly robust, fine adjustment for confounding, lots of confounders (especially drugs which were important) were measured.
% We also have more than two exposures.
Our study has several strengths. By using a doubly-robust estimation technique, and by accurately adjusting for thousands of plausible confounders, we minimized the bias in our estimates of the effect of hospital care on readmissions. Our work suggests that the difference the bias reduction was not trivial; in assessing the effect of hospitals on readmission, it has a substantive effect as well. Also, since we did not have to restrict our cohort to a single healthcare insurance network, we had a large cohort of patients from all socioeconomic classes. Because we had complete access to all hospital visits in the province, we could accurately measure which patients were readmitted.

% Prediciton vs inference.
Other hospital readmission studies have applied machine learning algorithms to readmission data to develop predictive models, including one using the data used in this study.\supercite{hosseinzadeh_assessing_2013} Most studies, including our own, found relatively poor accuracy.  No study to our knowledge has used machine learning algorithms to draw causal inference target parameters. Our study demonstrates that while predictive models may not be very accurate, machine learning techniques can improve our ability to draw inference on target parameters.

% On the preventability of readmissions.
Some authors \supercite{kansagara_risk_2011, clark_are_1990} believe that by using readmission rates as a quality metric, we assume that readmissions are preventable. Hoping to develop a quality metric that compares preventable readmissions, some researchers have attempted to identify which individual readmissions were preventable. Some studies have clinicians classify individual readmissions as preventable \supercite{witherington_communication_2008, stanley_review_2008, ruiz_factors_2008}, despite evidence that clinicians cannot reliably measure preventability.\supercite{van_walraven_incidence_2011} Other studies use pairs of admission/readmission diagnosis codes that identify "potentially preventable" readmissions.\supercite{halfon_validation_2006} However, the proportion of those actually preventable among the "potentially" preventable differs among hospitals \supercite{van_walraven_proportion_2011}, meaning that potentially preventable readmissions are not an adequate proxy for preventable readmissions.\supercite{clarke_are_1990}

But to estimate the effect of an exposure (like hospital care) on an outcome (like readmission), we do not need to identify exactly which individuals would not have had the outcome if they were not exposed.\supercite{hernan_causal_2014}  Some readmissions are unpreventable: no matter where they were treated, they would be readmitted. If patients were randomized among different hospitals, the number of unpreventable readmissions would be (asymptotically) the same among all hospitals, and any difference in readmission rates would be the "preventable proportion". Since the patients were not randomized to each hospital, we attempted to recreate that situation by controlling for confounding. Assuming that we have adequately controlled for confounding, we have estimated the independent effect of each hospital on readmission risk, without identifying whether \emph{individual} readmissions were preventable.
% End preventability.

% Sensitivity to the gbound / A practical positivity violation.
For some hospitals, our estimates for marginal risk were sensitive to the parameter $\delta$, which set a lower bound on the probability of exposure $g$. This suggests a practical positivity violation; some hospitals may not be admitting certain types of patients with high readmission risk, making it difficult to estimate what the risk of those patients would be if they had attended that hospital.  If we had a large enough sample, then we could very precisely estimate $g$, even for hospitals that a majority of our population had a very low probability of attending, and then precisely recreate the counterfactual population. Unfortunately, we would need immense statistical power to estimate a model for $g$  that precisely: a very small absolute difference in probability will make a big difference in the pseudopopulation. We believe that the discovery of practical positivity violations is an important finding: observational data may not provide us with enough information to meaningfully compare certain hospitals.
% End positivity violations.

% No mediators.
Following other hospital readmission work, we did not include hospital length of stay as a risk factor for readmission. Although we have strong reason to believe that the length of stay may affect readmission risk (and varies between hospitals), it acts as a mediator between hospital care and readmission; if we included it in our models, we risk biasing our estimate of the hospital's effect on readmission. However, unlike many other hospital readmission studies, we also excluded all diagnoses and procedures that occurred during the hospital admission, because these covariates were also effectively mediators between hospital care and readmission.

% Competing risks.
The major competing risk for hospital readmission is death, but others include moving outside the study area, or admission to a hospital for a non-emergency reason. In our analysis, we did not account for these competing risks. If patients died within 30 days of discharge more often at one hospital than another, we could have biased our estimate of readmission risk. Similarly, if patients died during the hospitalization more often at some hospitals than others, it could have created a selection bias (left censorship) in which hospitals with better care were discharging sicker (but still living) patients, who would be more likely to be readmitted. Also, there is no special significance of 30 days in readmission, except for the fact that it is (recently) widely used as a cutoff. In future work, we plan to account for both left censorship and competing risks in a model that estimates the effect of hospital care on time-to-readmission.

% Readmission isn't always a bad thing. Maybe the probability of admission has changed.
Differing \emph{admission} practices can strongly affect the rates of readmission. Since most  (\Sexpr{round(readmitted.to.same.hosp,2)*100}\% in this cohort) patients who are readmitted within 30 days are readmitted to the same hospital that they were discharged from, a hospital that is more likely to admit patients will have a higher readmission rate. In some cases, like a major trauma, admission is certain, but in most cases, there is some variation in practice of how patients are admitted. In future work, we plan to study the effect of the probability of admission on readmission.

% The influenza cohort.
Entry to our cohort was dependent on having one diagnosis of a respiratory illness in an inpatient or outpatient setting. Respiratory illness was defined rather broadly, including extremely common diagnoses such as "cough". We expect that the the majority of 65-year-old patients who would be hospitalized would have at least one respiratory illness diagnosis in an outpatient setting. We cannot, however, exclude the possibility that parameter estimates were affected by selection bias with respect to the full population of 65-year-old patients.

% Using TMLE in general.
The effect of hospital care on readmissions is confounded by a vast spectrum of health-related states of the admitted patients. In the absence of a clear theoretical basis of the structure of that confounding, we can 1) identify relatively few, well-understood and measurable confounders to include in our model, or 2) forgo any theoretical understanding of the structure of confounding, and attempt to identify the broadest measurable set of even faintly plausible confounders. The first option has some advantages: in a situation where data collection is expensive, we it may not be plausible to measure thousands of variables. Additionally, by reducing the confounders to a well-understood few, the model gains credibility because it can be shown that the confounders are having the expected effect. Non-parametric techniques such as random forest don't allow us to look (easily) at the individual effects of the confounders, and even in a parametric model it would be difficult to analyze thousands of variables. We summarized the densities of the effects a few classes of variables in figure \ref{fig:variable_importance_by_model_and_class}, but this still does not allow the variable-by-variable analysis typical in an epidemiologic study. Also, by including many confounders we also risk \emph{inducing} bias, such as the M-bias. However, the recent availability of large scale healthcare administrative data has put us into the situation where the cost of data collection is relatively low. By using machine learning techniques like random forest, we also automatically fit multi-way interactions that we would be unlikely to explore in a model fit "by hand". Finally, because the structure of the confounding is unclear, we cannot assess if M-bias is present. We argue that in this situation, where we have a large data set, thousands of measurable confounders, and little understanding of the structure of confounding, the second option is more appropriate.

% Differential misclassification due to coding practices.
Despite a relatively standardized data collection process, some hospitals may have idiosyncratic code usage patterns, leading to differing specificity and sensitivity of some diagnostic and procedural codes. This possible differential misclassification could have biased our estimate of the parameters of interest.

% Adjustment for multiple measurements.
The unit of analysis in this study was the discharge, but each discharge was "clustered" within a patient. The expected within-cluster homogeneity could have biased our estimates of variance, and our parameter estimates. However, because the number of clusters (unique patients) was relatively high when compared to the sample size (the number of discharges), we do not expect that our parameter or variance estimates to be biased very strongly.

% Why random forest?
Beside random forest, we could have used many other machine learning techniques on these data, many of which we explored in other work.\supercite{hosseinzadeh_assessing_2013}  Also, some ensemble machine learning techniques, (in particular SuperLearner which is commonly used with TMLE), are available, that combine any number of other machine learning techniques. We found that in these data, ensemble learning techniques were too computationally expensive. We selected random forest because of its relative simplicity, and because our variables were nearly all binary, for which decision trees are particularly suitable.

% Calibration
Calibration of the random forest vote proportions in the $Q$ model strongly affected estimates of our parameter of interest in the $Q^*$ update step. Other articles using non-parametric techniques typically combined them with other models in an ensemble learner (like SuperLearner). The final step in (many) ensemble learners is to combine all the probability estimates in parametric model, which would effectively calibrate the probabilities. In our study, a single, non-parametric technique was used, so an additional, separate calibration step was necessary to convert the ranking scores into a probability estimate.

% Crudeness
Hospital readmissions can be a relatively crude proxy for quality of care, but they can still provide valuable insight. In a seminal research article on quality of care measures, Donabedian writes: "But how precise do estimates of quality have to be? At least the better methods have been adequate for the administrative and social policy purposes that have brought them into being. The search for perfection should not blind one to the fact that present techniques of evaluating quality, crude as they are, have revealed a range of quality from outstanding to deplorable."\supercite{donabedian_evaluating_1966} Our work suggests that, when finely adjusted for confounding, hospital readmissions reveal wide differences in hospital quality of care.

\printbibliography

\section{Appendix}
% What does the accuracy of the random forest model look like as the number of trees grows for both the Q model and the G model?
\begin{figure}[H]
    \includegraphics{../figures/error_rate_for_hospital_choice.png}
    \caption[Error rate for random forest model of hospital choice.]
      {Error rate for both random forest models of hospital choice ($g$) and readmission ($Q$) as a function of the number of trees grown. For each admission, only out-of-bag trees were used to predict the given outcome.}
    \label{fig:error_rate_for_hospital_choice}
\end{figure}


% What does the accuracy of the random forest model look like as the number of trees grows for both the Q model and the G model?
\begin{figure}[H]
    \includegraphics{../figures/dist_g_zoomed.png}
    \caption[Histogram of the probability of exposure ($g$), restricted to the range of (0,0.05). The bin width is 0.005. The dotted red lines indicate the two values of $\delta$ used in tables 1, 2, and 3.]
      {Histogram of the probability of exposure ($g$), restricted to the range of (0,0.05). The bin width is 0.005. The dotted red lines indicate the two values of $\delta$ used in tables 1, 2, and 3.}
    \label{fig:dist_g_zoomed}
\end{figure}
\end{document}
